{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Enhancer.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API\")\n",
    "GOOGLE_SEARCH_API_KEY = os.getenv(\"GOOGLE_SEARCH_API\")\n",
    "SEARCH_ENGINE_ID = os.getenv(\"SEARCH_ENGINE_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Enhancer:\n",
    "    \"\"\"Class to enhance search queries and content using Gemini AI.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        if not GEMINI_API_KEY:\n",
    "            raise ValueError(\"Gemini API key is missing! Add it to .env\")\n",
    "\n",
    "        self.client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "    def enhance_query(self, original_query: str) -> str:\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Optimize this search query for maximum relevant web search results:\n",
    "        {original_query}\n",
    "        Return ONLY ONE search query.\n",
    "        \"\"\"\n",
    "\n",
    "        response = self.client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=[prompt]\n",
    "        )\n",
    "        return response.text\n",
    "\n",
    "    def enhance_content(self, original_content: str) -> str:\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        OPTIMIZE and SUMMARIZE this content for maximum relevance and clarity:\n",
    "\n",
    "        {original_content}\n",
    "        \n",
    "        DO NOT miss out on any important information.\n",
    "        DO NOT add any new information.\n",
    "        DO NOT change the meaning of the content.\n",
    "        \"\"\"\n",
    "\n",
    "        response = self.client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=[prompt]\n",
    "        )\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Url-Search.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def websearch_url(enhanced_query, num_results=3): #Returns a list of URLs using Google Search API!\n",
    "\n",
    "    search_url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    \n",
    "    params = {\n",
    "        \"q\": enhanced_query,  \n",
    "        \"key\": GOOGLE_SEARCH_API_KEY,  \n",
    "        \"cx\": SEARCH_ENGINE_ID,  \n",
    "        \"num\": num_results\n",
    "    }\n",
    "    \n",
    "    response = requests.get(search_url, params=params)\n",
    "    results = response.json()\n",
    "\n",
    "    if \"items\" in results:\n",
    "        return [item[\"link\"] for item in results[\"items\"]]\n",
    "    else:\n",
    "        return [\"No results found\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Scrapper.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re \n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from firecrawl import FirecrawlApp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv \n",
    "load_dotenv()\n",
    "\n",
    "FIRECRAWL_API = os.getenv(\"FIRECRAWL_API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraper:\n",
    "    def __init__(self):\n",
    "        if not FIRECRAWL_API:\n",
    "            raise ValueError(\"Firecrawl API key is missing! Add it to .env\")\n",
    "\n",
    "        self.app = FirecrawlApp(api_key=FIRECRAWL_API)\n",
    "        self.output_dir = \"Sessions\"\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def clean_text(self, content: str) -> str:\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "        extracted_text = []\n",
    "\n",
    "        for heading in soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]):\n",
    "            extracted_text.append(f\"\\n{heading.name.upper()}: {heading.get_text(strip=True)}\")\n",
    "\n",
    "        for paragraph in soup.find_all(\"p\"):\n",
    "            extracted_text.append(paragraph.get_text(strip=True))\n",
    "\n",
    "        for li in soup.find_all(\"li\"):\n",
    "            extracted_text.append(f\"‚Ä¢ {li.get_text(strip=True)}\")\n",
    "\n",
    "        for table in soup.find_all(\"table\"):\n",
    "            for row in table.find_all(\"tr\"):\n",
    "                cells = [cell.get_text(strip=True) for cell in row.find_all([\"th\", \"td\"])]\n",
    "                extracted_text.append(\" | \".join(cells))\n",
    "\n",
    "        return \"\\n\".join(extracted_text) if extracted_text else \"No relevant text found.\"\n",
    "\n",
    "    def scrape(self, url: str) -> str:\n",
    "        print(f\"\\nüîç Scraping: {url}\")\n",
    "\n",
    "        scrape_result = self.app.scrape_url(url, params={'formats': ['html']})\n",
    "        if 'html' not in scrape_result or not scrape_result['html']:\n",
    "            print(\"‚ùå No HTML content found.\")\n",
    "            return [False,None]\n",
    "\n",
    "        filtered_text = self.clean_text(scrape_result['html'])\n",
    "\n",
    "        file_path = os.path.join(self.output_dir, \"Cleaned.txt\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(filtered_text)\n",
    "\n",
    "        print(f\"‚úÖ Saved: {file_path}\")\n",
    "        return [True,filtered_text]\n",
    "\n",
    "    def crawl(self, url: str, depth: int = 1, limit: int = 100) -> str:\n",
    "        print(f\"\\nüîç Crawling: {url} (Depth: {depth})\")\n",
    "\n",
    "        crawl_status = self.app.crawl_url(\n",
    "            url,\n",
    "            params={\n",
    "                'limit': limit,\n",
    "                'scrapeOptions': {'formats': ['html']}\n",
    "            },\n",
    "            poll_interval=30\n",
    "        )\n",
    "\n",
    "        if 'pages' not in crawl_status:\n",
    "            print(\"‚ùå No pages found!\")\n",
    "            return [False,None]\n",
    "\n",
    "        all_text = []\n",
    "\n",
    "        for i, page in enumerate(crawl_status['pages']):\n",
    "            page_url = page['url']\n",
    "            print(f\"üìÑ Scraping page {i+1}: {page_url}\")\n",
    "\n",
    "            page_data = self.app.scrape_url(page_url, params={'formats': ['html']})\n",
    "            if 'html' in page_data and page_data['html']:\n",
    "                filtered_text = self.clean_text(page_data['html'])\n",
    "                all_text.append(filtered_text)\n",
    "\n",
    "        merged_content = \"\\n\\n\".join(all_text)\n",
    "\n",
    "        file_path = os.path.join(self.output_dir, \"Cleaned.txt\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(merged_content)\n",
    "\n",
    "        print(f\"‚úÖ Saved: {file_path}\")\n",
    "        return [True, merged_content]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
