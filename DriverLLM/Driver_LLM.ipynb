{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcjFGTvrC1Pv",
        "outputId": "6a08c126-8b81-4c2e-921b-33abe9a53582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests pandas datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from google.colab import files\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "m296DiGEDCyR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "GROQ_URL = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "\n",
        "# Output\n",
        "JSON_FILE_PATH = \"driver_llm_responses.json\"\n",
        "\n",
        "if os.path.exists(JSON_FILE_PATH) and os.path.getsize(JSON_FILE_PATH) > 0:\n",
        "    with open(JSON_FILE_PATH, \"r\", encoding=\"utf-8\") as file:\n",
        "        stored_responses = json.load(file)\n",
        "else:\n",
        "    stored_responses = []"
      ],
      "metadata": {
        "id": "lfeuK5GoDKBp"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Datasets and Fields\n",
        "HF_DATASETS = {\n",
        "    \"shreyanmitra/OpenEndedLLMPrompts\": \"question\",\n",
        "    \"sewon/ambig_qa\": \"question\",\n",
        "    \"aporia-ai/rag_hallucinations\": \"question\",\n",
        "    \"lasha-nlp/HALoGEN-prompts\": \"prompt\",\n",
        "    \"Cleanlab/FinQA-hallucination-detection\": \"query\",\n",
        "    \"tourist800/LLM-Hallucination-Detection-complex-mathematics\": \"Problem Statement\"\n",
        "}\n",
        "JSON_DATASETS = {\n",
        "    \"HaluEval\": (\"https://github.com/RUCAIBox/HaluEval/raw/main/data/general_data.json\", \"user_query\")\n",
        "}\n",
        "\n",
        "# Rate Limit Handling\n",
        "MAX_REQUESTS_PER_MINUTE = 30\n",
        "PAUSE_DURATION = 60 / MAX_REQUESTS_PER_MINUTE\n",
        "MAX_TOKENS_PER_MINUTE = 6000\n",
        "\n",
        "PENDING_QUERIES = []\n",
        "token_usage = 0\n",
        "start_time = time.time()"
      ],
      "metadata": {
        "id": "nWSLX5upDOQu"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GROQ API\n",
        "def query_groq(prompt):\n",
        "    \"\"\"Sends a request to the Groq API and handles rate limit errors.\"\"\"\n",
        "    global token_usage\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"llama-3.1-8b-instant\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "    }\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {GROQ_API_KEY}\"\n",
        "    }\n",
        "\n",
        "    response = requests.post(GROQ_URL, headers=headers, json=payload)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        response_data = response.json()\n",
        "        llm_response = response_data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
        "        token_usage += response_data.get(\"usage\", {}).get(\"total_tokens\", 0)\n",
        "        return llm_response\n",
        "\n",
        "    elif response.status_code == 429:\n",
        "        error_message = response.json().get(\"error\", {}).get(\"message\", \"\")\n",
        "        print(f\"Rate limit hit: {error_message}\")\n",
        "\n",
        "        wait_time = extract_wait_time(error_message)\n",
        "        PENDING_QUERIES.append(prompt)\n",
        "\n",
        "        print(f\"Storing query. Will retry in {wait_time} seconds...\")\n",
        "        time.sleep(wait_time)\n",
        "        return None\n",
        "\n",
        "    else:\n",
        "        return f\"Error: {response.status_code}, {response.text}\"\n"
      ],
      "metadata": {
        "id": "MjGO5LG_DUqh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_wait_time(error_message):\n",
        "    \"\"\"Extracts wait time from the error message (if rate limited).\"\"\"\n",
        "    try:\n",
        "        parts = error_message.split(\"Please try again in \")\n",
        "        wait_time = float(parts[1].split(\"ms\")[0].strip()) / 1000\n",
        "        return max(wait_time, 1)\n",
        "    except:\n",
        "        return 60"
      ],
      "metadata": {
        "id": "xHUfAumPDaIv"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_pending_queries():\n",
        "    \"\"\"Retries any stored queries after all normal queries are processed.\"\"\"\n",
        "    global PENDING_QUERIES\n",
        "    if PENDING_QUERIES:\n",
        "        print(f\"\\nRetrying {len(PENDING_QUERIES)} stored queries...\")\n",
        "        for prompt in PENDING_QUERIES:\n",
        "            response = query_groq(prompt)\n",
        "            if response:\n",
        "                stored_responses.append({\"query\": prompt, \"response\": response})\n",
        "        PENDING_QUERIES = []"
      ],
      "metadata": {
        "id": "f0pS-1VEDd-g"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_json_queries(url, field):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        queries = []\n",
        "        for line in response.text.strip().split(\"\\n\"):\n",
        "            try:\n",
        "                entry = json.loads(line)\n",
        "                if field in entry:\n",
        "                    queries.append(entry[field])\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Skipping malformed line: {e}\")\n",
        "\n",
        "        return random.sample(queries, min(200, len(queries)))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fetch {url}: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "bTsfawPbDh_R"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_hf_queries(dataset_name, field):\n",
        "    try:\n",
        "        dataset = load_dataset(dataset_name, split=\"train\")\n",
        "        queries = [entry[field] for entry in dataset if field in entry]\n",
        "        return random.sample(queries, min(200, len(queries)))\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fetch {dataset_name}: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "tHVgvy5_DlFA"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect Queries\n",
        "dataset_queries = []\n",
        "print(\"Fetching queries from datasets...\")\n",
        "\n",
        "for dataset, field in HF_DATASETS.items():\n",
        "    dataset_queries.extend(fetch_hf_queries(dataset, field))\n",
        "\n",
        "for dataset, (url, field) in JSON_DATASETS.items():\n",
        "    dataset_queries.extend(fetch_json_queries(url, field))\n",
        "\n",
        "print(f\"Total queries collected: {len(dataset_queries)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjsuOl2EDpkn",
        "outputId": "6a14836a-f558-48e3-8965-cd09a5276baf"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching queries from datasets...\n",
            "Total queries collected: 1400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query processing\n",
        "print(\"\\nProcessing queries with Groq API...\")\n",
        "processed_queries = {entry[\"query\"] for entry in stored_responses}\n",
        "\n",
        "for i, query in enumerate(dataset_queries):\n",
        "    if query in processed_queries:\n",
        "        continue\n",
        "\n",
        "    response = query_groq(query)\n",
        "\n",
        "    if response and not response.startswith(\"Error: 429\"):\n",
        "        stored_responses.append({\"query\": query, \"response\": response})\n",
        "\n",
        "    # Save after every 30 queries\n",
        "    if (i + 1) % 30 == 0:\n",
        "        print(f\"Processed {i + 1}/{len(dataset_queries)} queries...\")\n",
        "        with open(JSON_FILE_PATH, \"w\", encoding=\"utf-8\") as file:\n",
        "            json.dump(stored_responses, file, indent=4)\n",
        "\n",
        "    # Handle rate limit\n",
        "    elapsed_time = time.time() - start_time\n",
        "    if token_usage >= MAX_TOKENS_PER_MINUTE:\n",
        "        print(f\"Token limit reached ({token_usage}/6000). Waiting 60 seconds...\")\n",
        "        time.sleep(60)\n",
        "        start_time = time.time()\n",
        "        token_usage = 0\n",
        "    else:\n",
        "        time.sleep(PAUSE_DURATION)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jDXGfLKADtHf",
        "outputId": "f886d9cd-5ecb-4a8b-a6fc-79b30b2d6da6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing queries with Groq API...\n",
            "Token limit reached (6641/6000). Waiting 60 seconds...\n",
            "Token limit reached (6321/6000). Waiting 60 seconds...\n",
            "Processed 30/1400 queries...\n",
            "Token limit reached (6244/6000). Waiting 60 seconds...\n",
            "Token limit reached (6106/6000). Waiting 60 seconds...\n",
            "Token limit reached (6054/6000). Waiting 60 seconds...\n",
            "Processed 60/1400 queries...\n",
            "Token limit reached (6574/6000). Waiting 60 seconds...\n",
            "Token limit reached (6935/6000). Waiting 60 seconds...\n",
            "Token limit reached (6658/6000). Waiting 60 seconds...\n",
            "Processed 90/1400 queries...\n",
            "Token limit reached (6687/6000). Waiting 60 seconds...\n",
            "Token limit reached (6243/6000). Waiting 60 seconds...\n",
            "Token limit reached (7554/6000). Waiting 60 seconds...\n",
            "Processed 120/1400 queries...\n",
            "Token limit reached (6287/6000). Waiting 60 seconds...\n",
            "Token limit reached (6521/6000). Waiting 60 seconds...\n",
            "Token limit reached (6104/6000). Waiting 60 seconds...\n",
            "Processed 150/1400 queries...\n",
            "Token limit reached (6416/6000). Waiting 60 seconds...\n",
            "Token limit reached (6287/6000). Waiting 60 seconds...\n",
            "Processed 180/1400 queries...\n",
            "Token limit reached (6251/6000). Waiting 60 seconds...\n",
            "Token limit reached (6392/6000). Waiting 60 seconds...\n",
            "Token limit reached (6154/6000). Waiting 60 seconds...\n",
            "Processed 210/1400 queries...\n",
            "Processed 240/1400 queries...\n",
            "Token limit reached (6106/6000). Waiting 60 seconds...\n",
            "Processed 270/1400 queries...\n",
            "Token limit reached (6288/6000). Waiting 60 seconds...\n",
            "Processed 300/1400 queries...\n",
            "Token limit reached (6241/6000). Waiting 60 seconds...\n",
            "Processed 330/1400 queries...\n",
            "Processed 360/1400 queries...\n",
            "Token limit reached (6067/6000). Waiting 60 seconds...\n",
            "Processed 390/1400 queries...\n",
            "Token limit reached (6522/6000). Waiting 60 seconds...\n",
            "Processed 420/1400 queries...\n",
            "Token limit reached (6068/6000). Waiting 60 seconds...\n",
            "Processed 450/1400 queries...\n",
            "Token limit reached (6068/6000). Waiting 60 seconds...\n",
            "Processed 480/1400 queries...\n",
            "Token limit reached (6168/6000). Waiting 60 seconds...\n",
            "Processed 510/1400 queries...\n",
            "Token limit reached (6050/6000). Waiting 60 seconds...\n",
            "Processed 540/1400 queries...\n",
            "Token limit reached (6053/6000). Waiting 60 seconds...\n",
            "Processed 570/1400 queries...\n",
            "Token limit reached (6185/6000). Waiting 60 seconds...\n",
            "Processed 600/1400 queries...\n",
            "Token limit reached (6057/6000). Waiting 60 seconds...\n",
            "Token limit reached (6091/6000). Waiting 60 seconds...\n",
            "Processed 630/1400 queries...\n",
            "Token limit reached (6065/6000). Waiting 60 seconds...\n",
            "Processed 660/1400 queries...\n",
            "Token limit reached (6118/6000). Waiting 60 seconds...\n",
            "Token limit reached (10513/6000). Waiting 60 seconds...\n",
            "Processed 690/1400 queries...\n",
            "Token limit reached (6050/6000). Waiting 60 seconds...\n",
            "Processed 720/1400 queries...\n",
            "Token limit reached (6292/6000). Waiting 60 seconds...\n",
            "Token limit reached (6020/6000). Waiting 60 seconds...\n",
            "Processed 750/1400 queries...\n",
            "Token limit reached (6014/6000). Waiting 60 seconds...\n",
            "Processed 780/1400 queries...\n",
            "Token limit reached (6000/6000). Waiting 60 seconds...\n",
            "Processed 810/1400 queries...\n",
            "Token limit reached (6018/6000). Waiting 60 seconds...\n",
            "Processed 840/1400 queries...\n",
            "Token limit reached (6186/6000). Waiting 60 seconds...\n",
            "Processed 870/1400 queries...\n",
            "Token limit reached (6068/6000). Waiting 60 seconds...\n",
            "Processed 900/1400 queries...\n",
            "Token limit reached (6091/6000). Waiting 60 seconds...\n",
            "Processed 930/1400 queries...\n",
            "Token limit reached (6255/6000). Waiting 60 seconds...\n",
            "Processed 960/1400 queries...\n",
            "Token limit reached (6196/6000). Waiting 60 seconds...\n",
            "Processed 990/1400 queries...\n",
            "Token limit reached (10535/6000). Waiting 60 seconds...\n",
            "Token limit reached (6485/6000). Waiting 60 seconds...\n",
            "Token limit reached (6309/6000). Waiting 60 seconds...\n",
            "Processed 1020/1400 queries...\n",
            "Token limit reached (6540/6000). Waiting 60 seconds...\n",
            "Token limit reached (6770/6000). Waiting 60 seconds...\n",
            "Token limit reached (7658/6000). Waiting 60 seconds...\n",
            "Token limit reached (6182/6000). Waiting 60 seconds...\n",
            "Token limit reached (13028/6000). Waiting 60 seconds...\n",
            "Processed 1050/1400 queries...\n",
            "Token limit reached (6059/6000). Waiting 60 seconds...\n",
            "Token limit reached (8241/6000). Waiting 60 seconds...\n",
            "Token limit reached (13156/6000). Waiting 60 seconds...\n",
            "Token limit reached (6528/6000). Waiting 60 seconds...\n",
            "Token limit reached (7265/6000). Waiting 60 seconds...\n",
            "Processed 1080/1400 queries...\n",
            "Token limit reached (6146/6000). Waiting 60 seconds...\n",
            "Token limit reached (6243/6000). Waiting 60 seconds...\n",
            "Token limit reached (6264/6000). Waiting 60 seconds...\n",
            "Token limit reached (8163/6000). Waiting 60 seconds...\n",
            "Processed 1110/1400 queries...\n",
            "Token limit reached (6618/6000). Waiting 60 seconds...\n",
            "Token limit reached (7809/6000). Waiting 60 seconds...\n",
            "Token limit reached (6459/6000). Waiting 60 seconds...\n",
            "Token limit reached (6362/6000). Waiting 60 seconds...\n",
            "Token limit reached (6310/6000). Waiting 60 seconds...\n",
            "Processed 1140/1400 queries...\n",
            "Token limit reached (6681/6000). Waiting 60 seconds...\n",
            "Token limit reached (8160/6000). Waiting 60 seconds...\n",
            "Token limit reached (6099/6000). Waiting 60 seconds...\n",
            "Token limit reached (6954/6000). Waiting 60 seconds...\n",
            "Processed 1170/1400 queries...\n",
            "Token limit reached (10574/6000). Waiting 60 seconds...\n",
            "Token limit reached (6065/6000). Waiting 60 seconds...\n",
            "Token limit reached (7421/6000). Waiting 60 seconds...\n",
            "Token limit reached (7143/6000). Waiting 60 seconds...\n",
            "Token limit reached (6360/6000). Waiting 60 seconds...\n",
            "Processed 1200/1400 queries...\n",
            "Token limit reached (6082/6000). Waiting 60 seconds...\n",
            "Processed 1230/1400 queries...\n",
            "Token limit reached (6294/6000). Waiting 60 seconds...\n",
            "Token limit reached (6181/6000). Waiting 60 seconds...\n",
            "Processed 1260/1400 queries...\n",
            "Token limit reached (6087/6000). Waiting 60 seconds...\n",
            "Token limit reached (6110/6000). Waiting 60 seconds...\n",
            "Processed 1290/1400 queries...\n",
            "Rate limit hit: Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jqrk014ffw3vpgt7xgd8cc4w` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500261, Requested 15. Please try again in 47.7962s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing\n",
            "Storing query. Will retry in 60 seconds...\n",
            "Rate limit hit: Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jqrk014ffw3vpgt7xgd8cc4w` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500345, Requested 20. Please try again in 1m3.1324s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing\n",
            "Storing query. Will retry in 60 seconds...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-c9c3034318a9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_groq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error: 429\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-f9cfa3e360b8>\u001b[0m in \u001b[0;36mquery_groq\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Storing query. Will retry in {wait_time} seconds...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# retry any stored queries\n",
        "process_pending_queries()"
      ],
      "metadata": {
        "id": "7-utg_paDz_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out any remaining 429 errors before saving\n",
        "stored_responses = [entry for entry in stored_responses if not entry[\"response\"].startswith(\"Error: 429\")]\n",
        "\n",
        "# Save cleaned responses\n",
        "with open(JSON_FILE_PATH, \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(stored_responses, file, indent=4)\n",
        "\n",
        "print(f\"All {len(stored_responses)} valid queries processed and saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlWzRBERD1pf",
        "outputId": "bec5fc9b-1abd-4c3e-b853-7da21adbfd6b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All 1300 valid queries processed and saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(JSON_FILE_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "DmWLOXxgD3be",
        "outputId": "eb979973-d925-478d-c173-985e67962666"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_94e7be3c-4d34-43ea-96a2-bc8a4aa315bf\", \"driver_llm_responses.json\", 2078795)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}